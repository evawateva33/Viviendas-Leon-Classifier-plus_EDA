{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VL_Classifier",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1jBkpjDT3NM"
      },
      "source": [
        "# 👋 Introduction\n",
        "\n",
        "### Who are we? \n",
        "\n",
        "We are Berkeley undergraduates working with Viviendas Leon, a nonprofit dedicated to eliminating rural poverty in Nicaragua and Guatemala. \n",
        "\n",
        "`Team lead`: Elda Pere\n",
        "\n",
        "`Team members`: Lauren Faulds, Chase Elements, Barnett (Adam) Yang, Kathryn (Katie) Byers, Eva Sidlo, Kelly Trinh \n",
        "\n",
        "\n",
        "### Questions to address: \n",
        "Given location, soil, and weather data, which crops should a farmer plant that would be most resilient to disease?\n",
        "\n",
        "### Dataset description: \n",
        "`Data source`: Viviendas Leon \n",
        "\n",
        "It contains information on crop disease percentage, crop conditions, and any recommendations made. We also scraped weather data, which includes dew point, temperature, percipitation data, and more, and append to our dataset. \n",
        " \n",
        "### Objectives: \n",
        "1. Clean the data to account for missing value, inconsistent names, translate Spanish to English, and scrap weather data to supplement the dataset. \n",
        "\n",
        "2. Perform exploratory data analysis to find trends between crop type, effectiveness of recommendations, and geographical area, and disease percentage.\n",
        "\n",
        "3. Build a predictor with the following parameters:\n",
        "- Input: soil, temperature, and weather condition\n",
        "- Output: top 3 specific crops and the best general type of crops that are best for the given. This model works with 4 general crop types: `fruits`, `vegetables`, `legumes & seeds`, and `grasses`. Specific crops example are papayas, tomatoes, onions, etc. \n",
        "\n",
        "We will build a scoring system and a machine learning model. **Note: both the scoring system and the machine learning model with rank the specific crops and assign each of the crops a score based on how suitable it is for the given weather conditions. From this rank and score, we will extract out the top 3 specific crops and the general type of crop.** Our goal is to combine the predictions of the scoring system and the machine learning model.\n",
        "\n",
        "The use case of this model will be for Viviendas Leon to input their own real time data and the predictor will output the above predictions. These predictions will help the organization come up with better recommendations to farmers that the organization works with. \n",
        "\n",
        "### Outline of this notebook:\n",
        "1. Brief data cleaning  \n",
        "2. Scoring system \n",
        "3. Machine learning model\n",
        "4. Predictor function that combines the scoring system and machine learning model to create the predict mentioned in objective number 3. \n",
        "5. Appendix \n",
        "\n",
        "  (sections are not placed in the order they are executed)\n",
        "\n",
        "  5.1 Machine learning model selection \n",
        "\n",
        "  5.2 Hyperparameter tuning\n",
        "\n",
        "  5.3 Data processing \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jpwSYoNFBi4"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T52YEBZGPYYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a198f87-fb03-430b-9698-ad411b8d2c3e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import (drive, files)\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, make_scorer\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import compute_class_weight, compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler \n",
        "from collections import defaultdict\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDXzDykw4xCj"
      },
      "source": [
        "### 👀 Read Data\n",
        "\n",
        "`VL_farm_geo_w.csv` is the dataset that went through initial cleaning. \n",
        "\n",
        "Each row represents a visit from Viviendas Leon to a family. The row has information on the crop (condition, percent disease, type of crop, and weather conditions such as dew point, heat index, etc.\n",
        "\n",
        "\n",
        "The data spans from 2017 to 2021. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yhTLQhxPadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "f6886aff-c5fc-4b5f-b27f-650fdd3b2661"
      },
      "source": [
        "# connect to the drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive\"\n",
        "chases_path = path + \"/MyDrive\"\n",
        "kellys_path = path + \"/MyDrive/VL_Data/Farming_Data\"\n",
        "os.chdir(chases_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fe9e3c40a546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# connect to the drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/gdrive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchases_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/MyDrive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkellys_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/MyDrive/VL_Data/Farming_Data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    290\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "Z9px53Us3mc6",
        "outputId": "8a92b717-e33a-4119-fa95-0d3c39475b71"
      },
      "source": [
        "#df = pd.read_csv('/content/gdrive/MyDrive/VL_farm.csv') # read Kelly and Lauren's updated data\n",
        "\n",
        "df= pd.read_csv('VL_farm_geo_w.csv', index_col=0)\n",
        "\n",
        "\n",
        "# translate Spanish to English\n",
        "def season(month):\n",
        "    if month == 12 or month == 1 or month == 2:\n",
        "        return \"Winter\"\n",
        "    elif month == 3 or month == 4 or month == 5:\n",
        "        return \"Spring\"\n",
        "    elif month == 6 or month == 7 or month == 8:\n",
        "        return \"Summer\"\n",
        "    elif month == 9 or month == 10 or month == 11:\n",
        "        return \"Fall\"\n",
        "df[\"Season visited\"] = df[\"Month visited\"].apply(season)\n",
        "\n",
        "# 'Bueno'== good, '0'== null, 'Promedio'==average, 'Excelente'==excellent, 'Pobre'==poor, 'Excel', 'crisopa'\n",
        "df['Condition'] = df['Condition'].replace(['Excel'], 'Excelente')\n",
        "df['Condition'] = df['Condition'].replace(['Excelente'], \"excellent_cond\")\n",
        "df['Condition'] = df['Condition'].replace(['Promedio'], 'average_cond')\n",
        "df['Condition'] = df['Condition'].replace(['Bueno'], 'good_cond')\n",
        "df['Condition'] = df['Condition'].replace(['Pobre'], 'poor_cond')\n",
        "df['Condition'] = df['Condition'].replace(['crisopa'], 'bad?_cond')\n",
        "\n",
        "df['Condition'] = df['Condition'].replace([0], 'N/A_cond')\n",
        "df['Seedling_or_transplanted'].unique()\n",
        "df['Seedling_or_transplanted'] = df['Seedling_or_transplanted'].replace(['Almácigo'], 'seedling')\n",
        "df['Seedling_or_transplanted'] = df['Seedling_or_transplanted'].replace(['Transplantado'], 'transplanted')\n",
        "df['Seedling_or_transplanted'] = df['Seedling_or_transplanted'].replace(['Sin germinar'], 'transplanted')\n",
        "df['Seedling_or_transplanted'] = df['Seedling_or_transplanted'].replace(['Fructificacion'], 'fruitification')\n",
        "df['Seedling_or_transplanted'] = df['Seedling_or_transplanted'].replace(['Produccion'], 'production')\n",
        "df\n",
        "\n",
        "#drop rows with unsual values \n",
        "df = df[df.Condition != 'bad?_cond']\n",
        "df = df[df['Seedling_or_transplanted'] != 'fruitification']\n",
        "df = df[df['Seedling_or_transplanted'] != 'production']\n",
        "\n",
        "# Correct some crop spellings\n",
        "df['Crop'] = df['Crop'].replace(['Calabasa'], 'Calabaza')\n",
        "df['Crop'] = df['Crop'].replace(['Caña'], 'Caña de azucar')\n",
        "df['Crop'] = df['Crop'].replace(['Verngena', 'Verenjena', 'verenjena', 'verengena'], 'Verengena')\n",
        "df['Crop'] = df['Crop'].replace(['Rabano'], 'Rábano')\n",
        "df['Crop'] = df['Crop'].replace(['zanahoria'], 'Zanahoria')\n",
        "\n",
        "# preview of the data\n",
        "print('Data Shape', df.shape)\n",
        "hide_location = (df.columns != \"Region\") & (df.columns != \"Community\") & (df.columns != \"location\") & (df.columns != \"longitude\") & (df.columns !=\"latitude\")\n",
        "df.loc[0:5, hide_location]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b8769f13e696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df = pd.read_csv('/content/gdrive/MyDrive/VL_farm.csv') # read Kelly and Lauren's updated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VL_farm_geo_w.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VL_farm_geo_w.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kVIXIApT2AH"
      },
      "source": [
        "### Data Preview\n",
        "\n",
        "Looking at ordinal variable Crop Condition and % Illness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFOt8poXfWZG"
      },
      "source": [
        "# 0 = No entry \n",
        "df['Condition'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Overview of condition of all the crops\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Condition\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-xPxl5V1U-E"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "dsh = sns.lineplot(x=\"Month visited\", y=\"% Disease\"\n",
        "             ,data=df)\n",
        "plt.title(\"% Disease of all the crops in each month of a year\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNmqX6GRq-fZ"
      },
      "source": [
        "plt.title(\"Count of types of crops\")\n",
        "plt.xlabel(\"Crop type\")\n",
        "plt.ylabel(\"Count\")\n",
        "df['Type'].value_counts().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdZCfBc5rIfx"
      },
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.title(\"Count of each crop type\")\n",
        "plt.xlabel(\"Specific crop\")\n",
        "plt.ylabel(\"Count\")\n",
        "ax = df['Crop'].value_counts().plot(kind='bar')\n",
        "#ax.set_xticks(df['Crop'].value_counts().values)\n",
        ";\n",
        "print('Crops to predict: ' , df['Crop'].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BomMpv54W3Na"
      },
      "source": [
        "# ⚙️ Feature Engineering\n",
        "\n",
        "This step determines which variables are not relevant for the model (using correlations, interview with staff, etc).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntbL7Vo2tleX"
      },
      "source": [
        "### Add Additional Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kARG1zdC2H-2"
      },
      "source": [
        "# Combined Wellness metric using % Illness and Condition\n",
        "\n",
        "def condition_percentage (row):\n",
        "   if row['Condition'] == 'excellent_cond' :\n",
        "      return 1\n",
        "   if row['Condition'] == 'good_cond' :\n",
        "      return .90\n",
        "   if row['Condition'] == 'average_cond':\n",
        "      return .80\n",
        "   if row['Condition']  == 'poor_cond':\n",
        "      return .70\n",
        "   if row['Condition'] == '0':\n",
        "      return 1\n",
        "   return .80\n",
        "\n",
        "def condition_wellness_columns(df):\n",
        "    df['Percent_Condition'] = df.apply(lambda row: condition_percentage(row), axis=1)\n",
        "    df['Percent_wellness'] = 100 - df['% Disease']\n",
        "    df['Wellness_Condition'] = df['Percent_wellness'] * df['Percent_Condition']\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20nlxXb9Pu8Z"
      },
      "source": [
        "df = condition_wellness_columns(df)\n",
        "\n",
        "\n",
        "# One hot encoding\n",
        "df = df.join(pd.get_dummies(df[\"Season visited\"], prefix=\"Season\"), how = 'outer')\n",
        "df = df.join(pd.get_dummies(df[\"Condition\"], prefix=\"Condition\"), how = 'outer')\n",
        "df = df.join(pd.get_dummies(df[\"Region\"], prefix=\"Region\"), how= 'outer')\n",
        "df = df.join(pd.get_dummies(df[\"location\"], prefix=\"Location\"), how = \"outer\")\n",
        "df = df.join(pd.get_dummies(df[\"Seedling_or_transplanted\"], prefix=\"Trans_or_seed\"), how = \"outer\")\n",
        "\n",
        "# Drop now one-hot encoded columns\n",
        "# df = df.drop([\"Season visited\", \"Condition\", \"Region\", \"Plague\", \"location\", \"Seedling_or_transplanted\", \"Organic recommendation\", \"Chemical recommendation\"], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHfmpdUzUtj"
      },
      "source": [
        "# For each crop, \n",
        "df1 = df[['Region', 'Crop']]\n",
        "df1.groupby(['Crop']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvg0Lcg4XhhU"
      },
      "source": [
        "# drop crops with only 1 count\n",
        "df1 = df1.drop(df1.index[[4173, 5141, 2249, 6539, 4317, 3817, 6403, 5476, 311, 6697, 3002, 2908, 4494]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUonqOc6QcFb"
      },
      "source": [
        "df.to_csv('vl_geow_f.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgl81SS5pzCM"
      },
      "source": [
        "# 💯 Scoring System\n",
        "\n",
        "Required files:\n",
        "- `VL_farm_geo_w.csv`: A cleaned version of the raw Excel datasets\n",
        "\n",
        "Steps:\n",
        "- Initialize a new scoring function with `score = init_score(df)`, where df is the `cleaned.csv` pandas dataframe\n",
        "    - You can also set df to be any dataframe with the same column names, but you may have to change the default values for crops, regions, and communities when you call the score function.\n",
        "- To create a new score object, call `result = score(region, community)`, where region and community are optional parameters\n",
        "- To get rankings of n crops, call `result.get_best_composite(n)` to get list of crops with best composite scores (includes percent diseased, condition, region, and community scores (if applicable)), `result.get_best_region(n)` to get list of crops with best region scores (if region was specified in the above step), and `result.get_best_community(n)` to get list of crops with best community scores (if community was specified in the above step).\n",
        "    - By default, n is the number of unique crops in the dataset\n",
        "    - Highest scoring crops are listed first in the returned list\n",
        "    \n",
        "Function Descriptions:\n",
        "- `get_best_composite`: Ranks crops using composite scores based on condition, percent diseased, region (if applicable), and community (if applicable). The ranking of each crop corresponds to its order in the returned array (i.e. best to worst order). Uses the dictionary `comp_scores` to inform its rankings (the higher the score, the better the crop's rank).\n",
        "- `get_best_region`: Ranks crops using composite scores based on region (if applicable). The ranking of each crop corresponds to its order in the returned array. Uses the dictionaries `reg_cond_scores` (for conditions) and `reg_dis_scores` (for percent diseased) to inform its rankings.\n",
        "- `get_best_community`: Ranks crops using composite scores based on community (if applicable). The ranking of each crop corresponds to its order in the returned array. Uses the dictionaries `com_cond_scores` (for conditions) and `com_dis_scores` (for percent diseased) to inform its rankings.\n",
        "- `get_best_type_composite`: Ranks crop types (e.g. \"Veg\", \"Grians\", etc.) using composite scores based on condition, percent diseased, region (if applicable), and community (if applicable). The ranking of each crop type corresponds to its order in the returned array. Uses the dictionary `type_comp_scores` to inform its rankings (the higher the score, the better the crop type's rank).\n",
        "- `get_best_type_region`: Ranks crop types using composite scores based on region (if applicable). The ranking of each crop type corresponds to its order in the returned array. Uses the dictionaries `type_reg_cond_scores` (for conditions) and `type_reg_dis_scores` (for percent diseased) to inform its rankings.\n",
        "- `get_best_type_community`: Ranks crop types using composite scores based on community (if applicable). The ranking of each crop type corresponds to its order in the returned array. Uses the dictionaries `type_com_cond_scores` (for conditions) and `type_com_dis_scores` (for percent diseased) to inform its rankings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w69dU-4U-iLW"
      },
      "source": [
        "class ScoreResult:\n",
        "    def __init__(\n",
        "        self, \n",
        "        comp_scores, \n",
        "        cond_scores, \n",
        "        per_dis_scores, \n",
        "        reg_cond_scores, \n",
        "        reg_per_dis_scores, \n",
        "        com_cond_scores, \n",
        "        com_per_dis_scores,\n",
        "        type_comp_scores,\n",
        "        type_cond_scores,\n",
        "        type_per_dis_scores,\n",
        "        type_reg_cond_scores,\n",
        "        type_reg_per_dis_scores,\n",
        "        type_com_cond_scores,\n",
        "        type_com_per_dis_scores,\n",
        "        crops,\n",
        "        types\n",
        "    ):\n",
        "        self.comp_scores = comp_scores\n",
        "        self.cond_scores = cond_scores\n",
        "        self.per_dis_scores = per_dis_scores\n",
        "        self.reg_cond_scores = reg_cond_scores\n",
        "        self.reg_dis_scores = reg_per_dis_scores\n",
        "        self.com_cond_scores = com_cond_scores\n",
        "        self.com_dis_scores = com_per_dis_scores\n",
        "        self.type_comp_scores = type_comp_scores\n",
        "        self.type_cond_scores = type_cond_scores\n",
        "        self.type_per_dis_scores = type_per_dis_scores\n",
        "        self.type_reg_cond_scores = type_reg_cond_scores\n",
        "        self.type_reg_dis_scores = type_reg_per_dis_scores\n",
        "        self.type_com_cond_scores = type_com_cond_scores\n",
        "        self.type_com_dis_scores = type_com_per_dis_scores\n",
        "        self.crops = crops\n",
        "        self.types = types\n",
        "        \n",
        "    def get_best_composite(self, n=None):\n",
        "        if n == None:\n",
        "            n = len(self.crops)\n",
        "        crops = self.crops.copy()\n",
        "        crops.sort(key=lambda x: -self.comp_scores[x])\n",
        "        return crops[:n]\n",
        "    \n",
        "    def get_best_region(self, n=None):\n",
        "        if n == None:\n",
        "            n = len(self.crops)\n",
        "        crops = self.crops.copy()\n",
        "        crops.sort(key=lambda x: -(self.reg_cond_scores[x] + self.reg_dis_scores[x]))\n",
        "        return crops[:n]\n",
        "    \n",
        "    def get_best_community(self, n=None):\n",
        "        if n == None:\n",
        "            n = len(self.crops)\n",
        "        crops = self.crops.copy()\n",
        "        crops.sort(key=lambda x: -(self.com_cond_scores[x] + self.com_dis_scores[x]))\n",
        "        return crops[:n]\n",
        "    \n",
        "    def get_best_type_composite(self, n=None):\n",
        "        if n == None:\n",
        "            n = len(self.types)\n",
        "        types = self.types.copy()\n",
        "        types.sort(key=lambda x: -self.type_comp_scores[x])\n",
        "        return types[:n]\n",
        "    \n",
        "    def get_best_type_region(self, n=None):\n",
        "        if n == None:\n",
        "            n = len(self.types)\n",
        "        types = self.types.copy()\n",
        "        types.sort(key=lambda x: -(self.type_reg_cond_scores[x] + self.type_reg_dis_scores[x]))\n",
        "        return types[:n]\n",
        "    \n",
        "    def get_best_type_community(self, n=None):\n",
        "        if n == None:\n",
        "            n = len(self.types)\n",
        "        types = self.types.copy()\n",
        "        types.sort(key=lambda x: -(self.type_com_cond_scores[x] + self.type_com_dis_scores[x]))\n",
        "        return types[:n]\n",
        "        \n",
        "def normalize(d, target=1.0):\n",
        "    raw = sum(d.values())\n",
        "    factor = target/raw\n",
        "    return {key:value*factor for key,value in d.items()}\n",
        "\n",
        "def init_score(df,  \n",
        "               crops=None, \n",
        "               types=None, \n",
        "               regions=None, \n",
        "               communities=None, \n",
        "               conds = None,\n",
        "               cond_weights={\n",
        "                   \"good_cond\": 1, \n",
        "                   \"Bueno\" : 1,\n",
        "                   \"excellent_cond\": 2, \n",
        "                   \"Excelente\": 2,\n",
        "                   \"Excel\": 2,\n",
        "                   \"average_cond\": -1, \n",
        "                   \"Promedio\": -1,\n",
        "                   \"poor_cond\": -2, \n",
        "                   \"Pobre\": -2,\n",
        "                   \"crisopa\": -2,\n",
        "                   0: 0, \n",
        "                   '0': 0}):\n",
        "    if crops == None:\n",
        "        crops = list(df[\"Crop\"].unique())\n",
        "    if regions == None:\n",
        "        regions = list(df[\"Region\"].unique())\n",
        "    if types == None:\n",
        "        types = list(df[\"Type\"].unique())\n",
        "    if communities == None:\n",
        "        communities = list(df[\"Community\"].unique())\n",
        "    if conds == None:\n",
        "        conds = list(df[\"Condition\"].unique())\n",
        "        \n",
        "        \n",
        "    def score(region=None, community=None):\n",
        "        if region != None and region not in regions:\n",
        "            raise ValueError(f\"region is not valid, valid inputs include: {', '.join(regions)}\")\n",
        "        if community != None and community not in communities:\n",
        "            raise ValueError(f\"community is not valid, valid inputs include: {', '.join(communities)}\")\n",
        "        \n",
        "        comp_scores = dict.fromkeys(crops, 0)\n",
        "        cond_scores = dict.fromkeys(crops, 0)\n",
        "        per_dis_scores = dict.fromkeys(crops, 0)\n",
        "        \n",
        "        reg_cond_scores = dict.fromkeys(crops, 0)\n",
        "        reg_per_dis_scores = dict.fromkeys(crops, 0)\n",
        "        \n",
        "        com_cond_scores = dict.fromkeys(crops, 0)\n",
        "        com_per_dis_scores = dict.fromkeys(crops, 0)\n",
        "        \n",
        "        for crop in crops:\n",
        "            cond_total = 0\n",
        "            n = 0\n",
        "            crop_df = df[df[\"Crop\"] == crop]\n",
        "            cond_counts = crop_df[\"Condition\"].value_counts().to_dict()\n",
        "            for cond in conds:\n",
        "                if cond in cond_counts:\n",
        "                    cond_total += cond_counts[cond] * cond_weights[cond]\n",
        "                    n += cond_counts[cond]\n",
        "            if n == 0:\n",
        "                cond_scores[crop] = 0\n",
        "            else:\n",
        "                cond_scores[crop] = cond_total / n\n",
        "            per_dis_scores[crop] = 100 - crop_df[\"% Disease\"].mean()\n",
        "        cond_scores = normalize(cond_scores)\n",
        "        per_dis_scores = normalize(per_dis_scores)\n",
        "        \n",
        "        if region != None:\n",
        "            region_df = df[df[\"Region\"] == region]\n",
        "            for crop in crops:\n",
        "                cond_total = 0\n",
        "                n = 0\n",
        "                crop_df = region_df[region_df[\"Crop\"] == crop]\n",
        "                cond_counts = crop_df[\"Condition\"].value_counts().to_dict()\n",
        "                for cond in conds:\n",
        "                    if cond in cond_counts:\n",
        "                        cond_total += cond_counts[cond] * cond_weights[cond]\n",
        "                        n += cond_counts[cond]\n",
        "                if n == 0:\n",
        "                    reg_cond_scores[crop] = 0\n",
        "                else:\n",
        "                    reg_cond_scores[crop] = cond_total / n\n",
        "                if isinstance(100 - crop_df[\"% Disease\"].mean(), np.float64):\n",
        "                    reg_per_dis_scores[crop] = 100 - crop_df[\"% Disease\"].mean()\n",
        "                else:\n",
        "                    reg_per_dis_scores[crop] = 0\n",
        "            reg_cond_scores = normalize(reg_cond_scores)\n",
        "            reg_per_dis_scores = normalize(reg_per_dis_scores)\n",
        "                \n",
        "        if community != None:\n",
        "            com_df = df[df[\"Community\"] == community]\n",
        "            for crop in crops:\n",
        "                cond_total = 0\n",
        "                n = 0\n",
        "                crop_df = com_df[com_df[\"Crop\"] == crop]\n",
        "                cond_counts = crop_df[\"Condition\"].value_counts().to_dict()\n",
        "                for cond in conds:\n",
        "                    if cond in cond_counts:\n",
        "                        cond_total += cond_counts[cond] * cond_weights[cond]\n",
        "                        n += cond_counts[cond]\n",
        "                if n == 0:\n",
        "                    com_cond_scores[crop] = 0\n",
        "                else:\n",
        "                    com_cond_scores[crop] = cond_total / n\n",
        "                if isinstance(100 - crop_df[\"% Disease\"].mean(), np.float64):\n",
        "                    com_per_dis_scores[crop] = 100 - crop_df[\"% Disease\"].mean()\n",
        "                else:\n",
        "                    com_per_dis_scores[crop] = 0\n",
        "            com_cond_scores = normalize(com_cond_scores)\n",
        "            com_per_dis_scores = normalize(com_per_dis_scores)\n",
        "        \n",
        "        for crop in crops:\n",
        "            comp_scores[crop] += cond_scores[crop] + per_dis_scores[crop]\n",
        "            if region != None:\n",
        "                comp_scores[crop] += reg_cond_scores[crop] + reg_per_dis_scores[crop]\n",
        "            if community != None:\n",
        "                comp_scores[crop] += com_cond_scores[crop] + com_per_dis_scores[crop]\n",
        "        comp_scores = normalize(comp_scores)\n",
        "        \n",
        "        \n",
        "        type_comp_scores = dict.fromkeys(types, 0)\n",
        "        type_cond_scores = dict.fromkeys(types, 0)\n",
        "        type_per_dis_scores = dict.fromkeys(types, 0)\n",
        "        \n",
        "        type_reg_cond_scores = dict.fromkeys(types, 0)\n",
        "        type_reg_per_dis_scores = dict.fromkeys(types, 0)\n",
        "        \n",
        "        type_com_cond_scores = dict.fromkeys(types, 0)\n",
        "        type_com_per_dis_scores = dict.fromkeys(types, 0)\n",
        "        \n",
        "        for _type in types:\n",
        "            cond_total = 0\n",
        "            n = 0\n",
        "            type_df = df[df[\"Type\"] == _type]\n",
        "            cond_counts = type_df[\"Condition\"].value_counts().to_dict()\n",
        "            for cond in conds:\n",
        "                if cond in cond_counts:\n",
        "                    cond_total += cond_counts[cond] * cond_weights[cond]\n",
        "                    n += cond_counts[cond]\n",
        "            if n == 0:\n",
        "                type_cond_scores[_type] = 0\n",
        "            else:\n",
        "                type_cond_scores[_type] = cond_total / n\n",
        "            type_per_dis_scores[_type] = 100 - type_df[\"% Disease\"].mean()\n",
        "        type_cond_scores = normalize(type_cond_scores)\n",
        "        type_per_dis_scores = normalize(type_per_dis_scores)\n",
        "        \n",
        "        if region != None:\n",
        "            region_df = df[df[\"Region\"] == region]\n",
        "            for _type in types:\n",
        "                cond_total = 0\n",
        "                n = 0\n",
        "                type_df = region_df[region_df[\"Type\"] == _type]\n",
        "                cond_counts = type_df[\"Condition\"].value_counts().to_dict()\n",
        "                for cond in conds:\n",
        "                    if cond in cond_counts:\n",
        "                        cond_total += cond_counts[cond] * cond_weights[cond]\n",
        "                        n += cond_counts[cond]\n",
        "                if n == 0:\n",
        "                    type_reg_cond_scores[_type] = 0\n",
        "                else:\n",
        "                    type_reg_cond_scores[_type] = cond_total / n\n",
        "                if isinstance(100 - type_df[\"% Disease\"].mean(), np.float64):\n",
        "                    type_reg_per_dis_scores[_type] = 100 - type_df[\"% Disease\"].mean()\n",
        "                else:\n",
        "                    type_reg_per_dis_scores[_type] = 0\n",
        "            type_reg_cond_scores = normalize(type_reg_cond_scores)\n",
        "            type_reg_per_dis_scores = normalize(type_reg_per_dis_scores)\n",
        "                \n",
        "        if community != None:\n",
        "            com_df = df[df[\"Community\"] == community]\n",
        "            for _type in types:\n",
        "                cond_total = 0\n",
        "                n = 0\n",
        "                type_df = com_df[com_df[\"Type\"] == _type]\n",
        "                cond_counts = type_df[\"Condition\"].value_counts().to_dict()\n",
        "                for cond in conds:\n",
        "                    if cond in cond_counts:\n",
        "                        cond_total += cond_counts[cond] * cond_weights[cond]\n",
        "                        n += cond_counts[cond]\n",
        "                if n == 0:\n",
        "                    type_com_cond_scores[_type] = 0\n",
        "                else:\n",
        "                    type_com_cond_scores[_type] = cond_total / n\n",
        "                if isinstance(100 - type_df[\"% Disease\"].mean(), np.float64):\n",
        "                    type_com_per_dis_scores[_type] = 100 - type_df[\"% Disease\"].mean()\n",
        "                else:\n",
        "                    type_com_per_dis_scores[_type] = 0\n",
        "            type_com_cond_scores = normalize(type_com_cond_scores)\n",
        "            type_com_per_dis_scores = normalize(type_com_per_dis_scores)\n",
        "        \n",
        "        for _type in types:\n",
        "            type_comp_scores[_type] += type_cond_scores[_type] + type_per_dis_scores[_type]\n",
        "            if region != None:\n",
        "                type_comp_scores[_type] += type_reg_cond_scores[_type] + type_reg_per_dis_scores[_type]\n",
        "            if community != None:\n",
        "                type_comp_scores[_type] += type_com_cond_scores[_type] + type_com_per_dis_scores[_type]\n",
        "        type_comp_scores = normalize(type_comp_scores)\n",
        "        \n",
        "        return ScoreResult(\n",
        "            comp_scores, \n",
        "            cond_scores, \n",
        "            per_dis_scores, \n",
        "            reg_cond_scores, \n",
        "            reg_per_dis_scores, \n",
        "            com_cond_scores, \n",
        "            com_per_dis_scores,\n",
        "            type_comp_scores,\n",
        "            type_cond_scores,\n",
        "            type_per_dis_scores,\n",
        "            type_reg_cond_scores,\n",
        "            type_reg_per_dis_scores,\n",
        "            type_com_cond_scores,\n",
        "            type_com_per_dis_scores,\n",
        "            crops,\n",
        "            types\n",
        "        )\n",
        "    \n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtkut-gJi8eF"
      },
      "source": [
        "# 🔥 Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z42-HgxgvhiT"
      },
      "source": [
        "We are building a model that is predicting which general crop type should be planted. There are four general crop types: fruits, vegetables, legumes and seeds, and grasses. We call this the general crop model.\n",
        "\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "* Features = percent disease, wellness condition (i.e a combination of percent disease and crop condition), weather conditions, and location. \n",
        "* Y = ranking for each specific crop and a score for each specific crop.  \n",
        "* Addressing imbalanced classes: the class legumes and seeds and class grasses are less represented in the dataset. Realistically, we want farmers to plant a variety of crops, so we want to avoid having the model favoring one type of crop over the other. Therefore, we implemented class balance weights. \n",
        "\n",
        "\n",
        "**Model selection and evaluation**\n",
        "* We are implementing a multi-class classification model. We will be choosing between XGBoost, random forest, one-vs-rest, logistic regression, k-nearest neighbors, and support vector machines. \n",
        "* Criteria for a good model: We also wish for the farmers to plant a variety of crops, so we will choose the model that has the highest accuracy, and also recommend a good mix of crops.  \n",
        "* We used AUC, precision, recall, F1 score, the confusion matrix, and 5-fold cross validation accuracy score to evaluate each of the models.\n",
        "\n",
        "**Hyperparameter tuning**\n",
        "\n",
        "We will use grid search to tune the parameters of the best model selected.\n",
        "\n",
        "Below are the code and results of the model selection and evaluation.\n",
        "\n",
        "\n",
        "**Multiclass Classification**\n",
        "\n",
        "Predict crops with highest probability of success given features. Success is defined by `Wellness_Condition`. Highest likelihood of success is dervied from outputted probabilities of the model\n",
        "\n",
        "**Feature Columns Used:**\n",
        "  - Weather: `DewPointC`, `HeatIndexC`, `WindChillC`, `sunHour`\n",
        "  - Season: `Season_Fall`, `Season_Spring`, `Season_Summer`, `Season_Winter`, `Month Visited`(?)\n",
        "  - Location Based: `Region_Goyena`, `Region_Troilo`\n",
        "\n",
        "**Metrics**\n",
        "- Weight on the features for the either the quality or illness\n",
        "- Make list from outputs, then aggregate lists\n",
        "\n",
        "- Composite score: \n",
        "  - Quality & percent disease\n",
        "    - When modeling can get rid of crops with illness, bad quality \n",
        "  - Model that maps crops to expected percent disease , maps other conditions conditions with season and location ( average percent disease in different locations )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF_VhE-dMxT3"
      },
      "source": [
        "## 😊 Final model chosen\n",
        "\n",
        "After the model selection process and tuning the hyperparameters, we've chosen the XGBoost model with default parameters to be the final machine learning model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7n0BoyG9teV"
      },
      "source": [
        "def remove_low_crops(df):\n",
        "  '''\n",
        "  Outputs a dataframe for crop strain modeling without crops with low representaion\n",
        "  '''\n",
        "  # Getting counts of crops in dataframe\n",
        "  crop_counts = df.groupby(['Crop']).size().sort_values(ascending=True)\n",
        "  # Selecting index crop names with less than 10 counts\n",
        "  low_crops = crop_counts[crop_counts < 10].index.tolist()\n",
        "  # filtering dataframe without\n",
        "  df_without = df[~df['Crop'].isin(low_crops)]\n",
        "  return df_without\n",
        "\n",
        "def training(predictors, target):\n",
        "  '''\n",
        "  Uses predictors and target to split and train model. No normaliser needed for xgboost\n",
        "  '''\n",
        "  classes = np.unique(target)\n",
        "  class_weight = compute_class_weight('balanced', classes, target)\n",
        "\n",
        "  xgboost_model = XGBClassifier(scale_pos_weight=class_weight)\n",
        "  xgboost_model.fit(predictors, target)\n",
        "  return xgboost_model\n",
        "\n",
        "def get_preds(model, conditions):\n",
        "  '''\n",
        "  Given model and the user conditions obtain the top class predictions from the model\n",
        "  Conditions: '% Disease' (set to 0), 'Wellness_Condition' (set to 100), \n",
        "  'HeatIndexC' (avg 30.74), 'DewPointC' (avg 20.66), 'WindChillC' (avg 28.10), 'sunHour' (avg 10.95), \n",
        "  'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', \n",
        "  'Region_Goyena', 'Region_Troilo'\n",
        "  '''\n",
        "  # if certain conditions aren't given , then default values \n",
        "  target_prediction = model.predict(conditions)\n",
        "  class_probas = model.predict_proba(conditions)[0].tolist()\n",
        "  model_classes = model.classes_\n",
        "  class_probabilities = list(zip(model_classes, class_probas))\n",
        "  class_probabilities.sort(reverse=True, key=lambda x:x[1])\n",
        "  top_classes = [every[0] for every in class_probabilities[:3]]\n",
        "  return top_classes\n",
        "\n",
        "# May want to do this with the whole dataset for maximum representation of imbalanced classes\n",
        "# Add argument for Crop_model=True or Type_model=True to get more specific for accuracies desired\n",
        "def class_assessment(model, predictors, target):\n",
        "    '''\n",
        "    Assess the roc auc score for all classes. Saves a list of crops with higher roc auc scores\n",
        "    Uses: Model, Training set of X & y\n",
        "    -- default dict module needed from collections package\n",
        "    '''\n",
        "    crop_scores = defaultdict(list)\n",
        "    classes = model.classes_\n",
        "    X_train, X_test, y_train, y_test = train_test_split(predictors.values, target.values, test_size=0.2, shuffle=True)\n",
        "    kf = KFold(n_splits=3, random_state=42)\n",
        "    for train_ind, val_ind in kf.split(X_train, y_train):\n",
        "\n",
        "      # Split train into validation sets\n",
        "        X_tr, y_tr = X_train[train_ind], y_train[train_ind]\n",
        "        X_val, y_val = X_train[val_ind], y_train[val_ind]\n",
        "        # Get roc auc score for each crop\n",
        "        for each in classes:\n",
        "            fpr, tpr, thresholds = roc_curve(y_val,  \n",
        "                model.fit(X_tr, y_tr).predict_proba(X_val)[:,1], pos_label = each)\n",
        "            auc = round(metrics.auc(fpr, tpr),2)\n",
        "            crop_scores[each].append(auc)\n",
        "\n",
        "        crop_auc = pd.DataFrame.from_dict(crop_scores, orient='index')\n",
        "        crop_auc['avg'] = crop_auc.mean(axis=1)\n",
        "        \n",
        "    crop_auc2 = crop_auc[crop_auc['avg'] > 0.5]\n",
        "    crop_auc2.drop(crop_auc.columns[[0, 1, 2]], axis=1, inplace=True)\n",
        "    crop_auc2.sort_values(by=['avg'], ascending=False, inplace=True)\n",
        "    return [crop_auc2, classes]\n",
        "\n",
        "def cherry_pick(func_predictions, model_predictions, well_classified_crops):\n",
        "    '''\n",
        "    Use Adam's functions to supplement model predictions\n",
        "    '''\n",
        "    safe_predictions = [x for x in model_predictions if x in well_classified_crops]\n",
        "    safe_predictions.extend(func_predictions)\n",
        "    return safe_predictions[:3]\n",
        "\n",
        "'''\n",
        "Function for ensembling results\n",
        "'''\n",
        "# def avg_preds(model_predictions, func_predictions):\n",
        "#   '''\n",
        "#   Ensembling results of model and function\n",
        "#   '''\n",
        "#   model_predsdf = pd.DataFrame.from_dict(model_preds, orient='index').sort_values(by=[0], ascending=False).reset_index().reset_index()\n",
        "#   model_predsdf.columns = ['rank', 'crop', 'rating']\n",
        "#   func_predsdf = pd.DataFrame.from_dict(function_results, orient='index').sort_values(by=[0], ascending=False).reset_index().reset_index()\n",
        "#   func_predsdf.columns = ['rank', 'crop', 'rating']\n",
        "#   comb_predsdf = func_predsdf.merge(model_predsdf, left_on='crop', right_on='crop')\n",
        "#   comb_predsdf['averaged_rank'] = (comb_predsdf['rank_x'] + comb_predsdf['rank_y']) / 2 \n",
        "#   comb_predsdf.sort_values(by=['averaged_rank'])\n",
        "#   return comb_predsdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foi-Fe5kzYv7"
      },
      "source": [
        "## Training, cross validation and testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YcOFSOwSX-a"
      },
      "source": [
        "# Training\n",
        "df = pd.read_csv('vl_geow_f.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlC3Kfx8cJf7"
      },
      "source": [
        "# 1) Remove the low crops\n",
        "df_less = remove_low_crops(df)\n",
        "\n",
        "# 2) Train the model\n",
        "predictorsc = df_less[['% Disease', 'Wellness_Condition', 'HeatIndexC', 'DewPointC', 'WindChillC', 'sunHour', 'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Region_Goyena', 'Region_Troilo']]\n",
        "targetc = df_less['Crop']\n",
        "\n",
        "predictorst = df[['% Disease', 'Wellness_Condition', 'HeatIndexC', 'DewPointC', 'WindChillC', 'sunHour', 'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Region_Goyena', 'Region_Troilo']]\n",
        "targett = df['Type']\n",
        "\n",
        "crop_model = training(predictorsc, targetc)\n",
        "type_model = training(predictorst, targett)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aExDjXQAk0Ly"
      },
      "source": [
        "crop_model.predict(testc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE3PlV79U2m8"
      },
      "source": [
        "## ➡️ Test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdhDTxMSobl"
      },
      "source": [
        "# Load the conditions into a dataframe\n",
        "some_conditions = [0, 100, 30.74, 20.66, 28.10, 10.95, 0, 0, 0, 0, 0, 0]\n",
        "columns_dict = {0: '% Disease', 1: 'Wellness_Condition', 2: 'HeatIndexC',\n",
        "                  3: 'DewPointC', 4: 'WindChillC', 5: 'sunHour',\n",
        "                  6: 'Season_Fall', 7: 'Season_Spring', 8: 'Season_Summer',\n",
        "                  9: 'Season_Winter', 10: 'Region_Goyena', 11: 'Region_Troilo'}\n",
        "some_conditions_df = pd.DataFrame(some_conditions).T.rename(columns=columns_dict)\n",
        "\n",
        "# Get the predictions from the model\n",
        "crop_preds = get_preds(crop_model, some_conditions_df)\n",
        "type_preds = get_preds(type_model, some_conditions_df)\n",
        "\n",
        "well_classified_categories = class_assessment(type_model, predictorst, targett)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaYjaTqz2VwG"
      },
      "source": [
        "well_classified_categories.reset_index().rename(columns={'index': 'Crop Type'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74DwRquNOcg"
      },
      "source": [
        "# 🎉 Final combined function\n",
        "\n",
        "This will be the function that combine the scoring system and the machine learning model. \n",
        "\n",
        "**How it works:**\n",
        "\n",
        "We will first use the machine learning model to output the ranks and the scores for the specific crop types. \n",
        "\n",
        "We will examine the AUC score for the recommended general crop type. If the score is below a certain threshold, then instead the ranks and scores that the machine learning model outputs, we use ranks and scores from the scoring system. If the score is above the threshold, then we will run a function that combines the ranks and scores the machine learning model predicts with the ranks and scores the scoring system predict. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlSy6Iy7yKR8"
      },
      "source": [
        "  import pickle\n",
        "  \n",
        "  predictorsc = df_less[['% Disease', 'Wellness_Condition', 'HeatIndexC', 'DewPointC', 'WindChillC', 'sunHour', 'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Region_Goyena', 'Region_Troilo']]\n",
        "  targetc = df_less['Crop']\n",
        "  \n",
        "  classes = np.unique(targetc)\n",
        "  class_weight = compute_class_weight('balanced', classes, targetc)\n",
        "\n",
        "  xgboost_model = XGBClassifier(scale_pos_weight=class_weight)\n",
        "  xgboost_model.fit(predictorsc, targetc)\n",
        "\n",
        "filename = 'finalized_model33.pkl'\n",
        "pickle.dump(xgboost_model, open(filename, 'wb'))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsGdxTrmNPbd"
      },
      "source": [
        "# Maybe useful? df_test_binary[\"Fare\"].fillna(df_test_binary.groupby(\"NameLen\")[\"Fare\"].transform(\"median\"), inplace=True)\n",
        "def final_function(Percent_Disease=0, Wellness_Condition=100, HeatIndexC=30.74,\n",
        "                   DewPointC=20.66, WindChillC=28.10, sunHour=10.95,\n",
        "                   Season_Fall=0, Season_Spring=0,\n",
        "                   Season_Summer=0, Season_Winter=0,\n",
        "                   Region_Goyena=0, Region_Troilo=0):\n",
        "  \"\"\"\n",
        "      NOTE: This function assumes that df is defined above!\n",
        "\n",
        "      The parameters are set to default values as specified by the model. Feel\n",
        "      free to pass in as many or as few of these parameters as necessary. This\n",
        "      function will return a string representing the model's recommendation.\n",
        "  \"\"\"\n",
        "  # 1) Remove the low crops\n",
        "  df_less = remove_low_crops(df)\n",
        "\n",
        "  # 2) Train the model\n",
        "  predictorsc = df_less[['% Disease', 'Wellness_Condition', 'HeatIndexC', 'DewPointC', 'WindChillC', 'sunHour', 'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Region_Goyena', 'Region_Troilo']]\n",
        "  targetc = df_less['Crop']\n",
        "\n",
        "  predictorst = df[['% Disease', 'Wellness_Condition', 'HeatIndexC', 'DewPointC', 'WindChillC', 'sunHour', 'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Region_Goyena', 'Region_Troilo']]\n",
        "  targett = df['Type']\n",
        "\n",
        "  crop_model = training(predictorsc, targetc)\n",
        "  type_model = training(predictorst, targett)\n",
        "\n",
        "  # 3) Put the passed in conditions into a dataframe\n",
        "  conditions = [Percent_Disease, Wellness_Condition, HeatIndexC, DewPointC,\n",
        "                WindChillC, sunHour, Season_Fall, Season_Spring, Season_Summer,\n",
        "                Season_Winter, Region_Goyena, Region_Troilo]\n",
        "  columns_dict = {0: '% Disease', 1: 'Wellness_Condition', 2: 'HeatIndexC',\n",
        "                  3: 'DewPointC', 4: 'WindChillC', 5: 'sunHour',\n",
        "                  6: 'Season_Fall', 7: 'Season_Spring', 8: 'Season_Summer',\n",
        "                  9: 'Season_Winter', 10: 'Region_Goyena', 11: 'Region_Troilo'}\n",
        "  conditions_df = pd.DataFrame(conditions).T.rename(columns=columns_dict)\n",
        "\n",
        "  # 4) Call the model\n",
        "  crop_preds = get_preds(crop_model, conditions_df)\n",
        "  type_preds = get_preds(type_model, conditions_df)\n",
        "  well_classified_crops = class_assessment(crop_model, predictorsc, targetc)\n",
        "  well_classified_crops.reset_index().rename(columns={'index': 'Crop'})\n",
        "  well_classified_crops = well_classified_crops.head(3)\n",
        "  well_classified_categories = class_assessment(type_model, predictorst, targett)\n",
        "  well_classified_categories.reset_index().rename(columns={'index': 'Crop Type'})\n",
        "  well_classified_categories = well_classified_categories.head(1)\n",
        "\n",
        "  # 5) Call the scoring system\n",
        "  score = init_score(df)\n",
        "  result = score()\n",
        "  if Region_Goyena == 1:\n",
        "    region = 'Goyena'\n",
        "  elif Region_Troilo == 1:\n",
        "    region = 'Troilo'\n",
        "  else:\n",
        "    region = None\n",
        "  score_func = init_score(df)\n",
        "  result = score_func(region)\n",
        "  high_score_crops = result.get_best_composite(n=3)\n",
        "  high_score_categories = result.get_best_type_composite(n=1)\n",
        "  if high_score_categories[0] == 'Veg':\n",
        "    high_score_categories[0] = 'Vegetable'\n",
        "\n",
        "  # 6) Add the results of the crop scoring to get the crop DF up to 3\n",
        "  crops_length = len(well_classified_crops)\n",
        "  if crops_length < 3:\n",
        "    high_score_crops_2D = []\n",
        "    for i in range(min(len(high_score_crops, 3 - crops_length))):\n",
        "      high_score_crops_2D.append([high_score_crops[i], None])\n",
        "    high_score_df = pd.DataFrame(new_high_score_crops, columns=['Crop', 'avg'])\n",
        "    well_classified_crops = well_classified_crops.append(high_score_df)\n",
        "  \n",
        "  # 7) Add the results of the crop type scoring if the category DF is 0 in len\n",
        "  if len(well_classified_categories) < 1:\n",
        "    high_score_types_2D = [[high_score_categories[0], None]]\n",
        "    well_classified_categories = pd.DataFrame(high_score_types_2D, columns=['Crop Type', 'avg'])\n",
        "  \n",
        "  # 8) Return the crop and category recommendations dataframes in a list\n",
        "  return [well_classified_crops, well_classified_categories]\n",
        "  \n",
        "final_function()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEnkhd4OxWkF"
      },
      "source": [
        "\n",
        "well_classified_crops = class_assessment(crop_model, predictorsc, targetc)\n",
        "well_classified_crops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q85cpjy-ud7Q"
      },
      "source": [
        "well_classified_crops[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H28VT5yEsqKm"
      },
      "source": [
        " # 📚 Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZNH_VNnZx-t"
      },
      "source": [
        "#### Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpSTw1XcAQ6B"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, shuffle=True)\n",
        "\n",
        "# data normalization using MinMaxScaler (standardscaler decreases accuracy due to nongaussian distribution)\n",
        "norm = MinMaxScaler().fit(X_train)\n",
        "X_train_norm = norm.transform(X_train)\n",
        "X_test_norm = norm.transform(X_test)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=42)\n",
        "\n",
        "classes = np.unique(target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUj7ogy6d6nw"
      },
      "source": [
        "# Checking for America's Next Top Models\n",
        "# cv_results = {}\n",
        "# result_table = pd.DataFrame(columns=['classifiers', 'accuracy'])\n",
        "\n",
        "models = {'xgboost': XGBClassifier(random_state=42,),\n",
        "        'logistic regression': LogisticRegression(solver=\"lbfgs\", random_state=42, multi_class=\"multinomial\"),\n",
        "        'KNN': KNeighborsClassifier( n_neighbors=5),\n",
        "        'decision tree': DecisionTreeClassifier(random_state=42),\n",
        "        'random forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "        'SVC': svm.SVC(random_state=42),\n",
        "        'one vs rest': OneVsRestClassifier(SVC(random_state=42)) }\n",
        "\n",
        "for model in models.items():\n",
        "  for train_ind, val_ind in kf.split(X_train_norm, y_train):\n",
        "    X_tr, y_tr = X_train_norm[train_ind], y_train.iloc[train_ind]\n",
        "    X_val, y_val = X_train_norm[val_ind], y_train.iloc[val_ind]\n",
        "    #fit model in dictionary with values\n",
        "    model[1].fit(X_tr, y_tr)\n",
        "    #predict\n",
        "    y_pred = model[1].predict(X_val)\n",
        "\n",
        "  #testing metrics\n",
        "    precision = precision_score(y_val, y_pred, average='weighted')\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "    # cv_results[model[0]] = (accuracy)\n",
        "    cm = confusion_matrix(y_val,y_pred)\n",
        "\n",
        "  #roc auc scores\n",
        "  for each in classes:\n",
        "      fpr, tpr, thresholds = roc_curve(y_val,  \n",
        "                      model[1].predict_proba(X_val)[:,1], pos_label = each) \n",
        "\n",
        "      auroc = round(metrics.auc(fpr, tpr),2)\n",
        "      print(each,'--AUC--->',auroc)\n",
        "\n",
        "  print(model[0], '\\n', 'accuracy score:', accuracy, '\\n', 'f1 score: ', f1, '\\n precision: ', precision)\n",
        "  ax= plt.subplot()\n",
        "  sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap='Blues');  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "  # labels, title and ticks\n",
        "  ax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \n",
        "  ax.set_title('Confusion Matrix'); \n",
        "  ax.xaxis.set_ticklabels(['Fruit', 'Grains', 'Legumes', 'Veg']); ax.yaxis.set_ticklabels(['Fruit', 'Grains', 'Legumes', 'Veg']);\n",
        "  plt.show()      \n",
        "  print('\\n') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk8wbTJyEC05"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87sIVNlDrI_Q"
      },
      "source": [
        "# XGBoost with oversampled data\n",
        "# over sampled train set\n",
        "oversample = RandomOverSampler(random_state=42)\n",
        "X_train_over, y_train_over = oversample.fit_resample(X_train_norm, y_train)\n",
        "\n",
        "\n",
        "xgboost_model_o = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                    colsample_bynode=1, colsample_bytree=1, gamma=0, learn_rate=0.2,\n",
        "                    learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
        "                    min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "                    nthread=None, objective='multi:softprob', random_state=42,\n",
        "                    reg_alpha=0, reg_lambda=1, sample_rate=0.8,\n",
        "                    seed=None, silent=None, subsample=1, verbosity=1)\n",
        "xgboost_model_o.fit(X_train_over, y_train_over)\n",
        "\n",
        "y_pred = xgboost_model_o.predict(X_test_norm)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4SwswDjEvfN"
      },
      "source": [
        "# XGBoost with tuned parameters\n",
        "# {'gamma': 1, 'learn_rate': 0.1, 'max_depth': 3, 'subsample': 0.9}\n",
        "tuned_xgboost_model = XGBClassifier(gamma=1, learn_rate=0.1, max_depth=3, subsample=0.9)\n",
        "tuned_xgboost_model.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "y_pred = tuned_xgboost_model.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-566h5EdEcbT"
      },
      "source": [
        "class_weight = compute_class_weight('balanced', classes, target)\n",
        "print(class_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ideO92htHovC"
      },
      "source": [
        "# Finding the best parameters for XGBoost\n",
        "def parameter_tune(clf, hyper_params):\n",
        "    # Type of scoring used to compare parameter combinations\n",
        "    acc_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "    # Run the grid search\n",
        "    grid_obj = GridSearchCV(clf, hyper_params, scoring=acc_scorer)\n",
        "    grid_obj.fit(X_train, y_train)\n",
        "    return grid_obj\n",
        "\n",
        "# Parameter combonations that the grid search will try\n",
        "hyper_params = {'max_depth': [3, 4, 5],\n",
        "                'learn_rate': [0.1, 0.09, 0.08, 0.07],\n",
        "                'subsample': [0.8, 0.9, 1],\n",
        "                'gamma': [0, 1, 5]}\n",
        "# Find the best parameters with GridSearchCV\n",
        "#grid_search_obj = parameter_tune(XGBClassifier(), hyper_params)\n",
        "#grid_search_obj.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li4q50TOLbQx"
      },
      "source": [
        "# Make the results into a dataframe\n",
        "#grid_search_df = pd.DataFrame(grid_search_obj.cv_results_)\n",
        "#grid_search_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7f6V4sX8ztB"
      },
      "source": [
        "# Print out the best parameters\n",
        "#grid_search_obj.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl6GKbFQ8yK8"
      },
      "source": [
        "# Get the best estimator\n",
        "#BEST_XG_CLF = grid_search_obj.best_estimator_\n",
        "#BEST_XG_CLF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfIDcpUbOtOc"
      },
      "source": [
        "# Print the classification_report of the best estimator\n",
        "#BEST_XG_CLF.fit(X_train, y_train)\n",
        "\n",
        "#y_pred = BEST_XG_CLF.predict(X_test)\n",
        "#accuracy_score(y_test, y_pred)\n",
        "\n",
        "#print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcmrRSeN9D98"
      },
      "source": [
        "predictors = df[['% Disease', 'Wellness_Condition', 'HeatIndexC', 'DewPointC', 'WindChillC', 'sunHour', 'Season_Fall', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Region_Goyena', 'Region_Troilo']]\n",
        "targetc = df['Crop']\n",
        "\n",
        "X_trainc, X_testc, y_trainc, y_testc = train_test_split(predictors, targetc, test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM7PC3Yu9Pqh"
      },
      "source": [
        "# Crops data \n",
        "\n",
        "xgboost_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                    colsample_bynode=1, colsample_bytree=1, gamma=0, learn_rate=0.2,\n",
        "                    learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
        "                    min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "                    nthread=None, objective='multi:softprob', random_state=42,\n",
        "                    reg_alpha=0, reg_lambda=1, sample_rate=0.8, scale_pos_weight=class_weight,\n",
        "                    seed=None, silent=None, subsample=1, verbosity=1)\n",
        "xgboost_model.fit(X_trainc, y_trainc)\n",
        "\n",
        "y_pred = xgboost_model.predict(X_testc)\n",
        "accuracy_score(y_testc, y_pred)\n",
        "\n",
        "print(classification_report(y_testc, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1TbSETY6zvx"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpHnFJ2T98Iw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeWVeyT0CobY"
      },
      "source": [
        "\n",
        "## ✏️ Data Preparation\n",
        "\n",
        "\n",
        "We produced a cleaned version of the data titled `VL_farm_geo_w.csv`.\n",
        "Below is a description of the initial data cleaning steps we took. \n",
        "\n",
        "The data we received from Viviendas Leon (VL) originally consisted of these files:\n",
        "\n",
        "1) corrected names of farmers VL worked with\n",
        "\n",
        "2) farming data 2017 - 2021 of the farmers \n",
        "\n",
        "3) coordinates of the families\n",
        "\n",
        "We further scraped weather information to add to our analysis. \n",
        "\n",
        "1) replaced all the family names with corrected names\n",
        "\n",
        "2) translated Spanish (the original language of the data) into English\n",
        "\n",
        "3) merged the farming data with weather and geocoordinate data\n",
        "\n",
        "Below are useful functions we used for data cleaning, and the code of our data cleaning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7amnEYwD4KN"
      },
      "source": [
        "# Load farming data, weather data, geo data\n",
        "\n",
        "# dropped last three rows (just empty)\n",
        "correct_names = pd.read_excel(\"210304_Full Participant List Farming Program 2017-2021.xlsx\").drop([51,52,53], axis=0)\n",
        "correct_names.columns = correct_names.loc[0,:]\n",
        "correct_names = correct_names.drop([0], axis=0)\n",
        "\n",
        "# load data\n",
        "data_17_19 = pd.read_excel(\"VL Huertos Familiares- Hoja de Datos (2017-2019).xlsx\")\n",
        "data_19_20 = pd.read_excel(\"VL Huertos Familiares- Hoja de Datos (2019-2020).xlsx\")\n",
        "\n",
        "# Lauren\n",
        "historic_w = pd.read_csv('w_historic.csv')\n",
        "families_coordinates = pd.read_csv('family_coordinates_api.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON92OKtRECKe"
      },
      "source": [
        "def combined_farming(dataframe1, dataframe2):\n",
        "  # Kelly\n",
        "  #drop empty column\n",
        "  dataframe1 = dataframe1.drop(\"Unnamed: 10\", axis=1)\n",
        "\n",
        "  '''Translate Spanish column names into English'''\n",
        "\n",
        "  translated_cols = ['Date visited','Auditor','Region','Community','Family visited','Present?',\n",
        "                      'Fruit','Fruit_Condition (seedling or transplanted)', 'Fruit_% Disease','Fruit_Condition',\n",
        "                      'Fruit_Plague','Fruit_Organic recommendation','Fruit_Chemical recommendation',\n",
        "                      'Vegetables','Veg_Condition (seedling or transplanted)','Veg_% Disease','Veg_Condition',\n",
        "                      'Veg_Plague','Veg_Organic recommendation','Veg_Chemical recommendation',\n",
        "                      'Legumes and seeds','LnS_Condition (seedling or transplanted)','LnS_% Disease',\n",
        "                      'LnS_Condition','LnS_Plague','LnS_Organic recommendation','LnS_Chemical recommendation',\n",
        "                      'Grasses','Grasses_Condition (seedling or transplanted)','Grasses_% Disease',\n",
        "                      'Grasses_Condition','Grasses_Plague','Grasses_Organic recommendation',\n",
        "                      'Grasses_Chemical recommendation',\n",
        "                      'Commentaries, additional remarks','Response, commentary follow up']\n",
        "  # rename columns \n",
        "  dataframe1.columns = translated_cols\n",
        "  dataframe2.columns = translated_cols\n",
        "\n",
        "  # drop first 3 rows (headings of table names)\n",
        "  dataframe1 = dataframe1.drop([0, 1, 2], axis=0) \n",
        "  dataframe2 = dataframe2.drop([0, 1, 2], axis=0)\n",
        "\n",
        "  # appending 2019-2020 to the bottom of 2017-2019 data\n",
        "  combined_data = dataframe1.append(dataframe2)\n",
        "\n",
        "  return combined_data\n",
        "\n",
        "def clean_farming(combined_data):\n",
        "\n",
        "  '''Preliminary cleaning for consistent data entry, correct names, and structuring\n",
        "  data so that each row is one observation for one crop.'''\n",
        "\n",
        "    # fill NaN with 0s\n",
        "    combined_data = combined_data.fillna(0)\n",
        "\n",
        "\n",
        "    # further cleaning\n",
        "    combined_data[\"Month visited\"] = combined_data[\"Date visited\"].dt.month\n",
        "    combined_data[\"Year visited\"] = combined_data[\"Date visited\"].dt.year\n",
        "    combined_data[\"Veg_% Disease\"] = combined_data[\"Veg_% Disease\"].replace(['35 %', ';4', ' '], [35, 4, 0])\n",
        "    combined_data[\"Fruit\"] = combined_data[\"Fruit\"].replace(['Piña', 'piña'], 'Pina')\n",
        "    combined_data[\"Legumes and seeds\"] = combined_data[\"Legumes and seeds\"].replace(\"Pipián\", \"Pipian\")\n",
        "    combined_data[\"Legumes and seeds\"] = combined_data[\"Legumes and seeds\"].replace(\"I\", \"None\")\n",
        "    combined_data[\"Legumes and seeds\"] = combined_data[\"Legumes and seeds\"].replace([\"Frijol Rojo\", \"Frijoles rojo\"], \"Frijol rojo\")\n",
        "    combined_data[\"Legumes and seeds\"] = combined_data[\"Legumes and seeds\"].replace([\"Frijoles blanco\", \"Frijol blanco\"])\n",
        "\n",
        "    # replace with corrected names\n",
        "    combined_data = combined_data.replace(correct_names.iloc[:,3].values, correct_names.iloc[:,4].values)\n",
        "\n",
        "    # structure data so that one row is one observation \n",
        "    overall_info = combined_data.iloc[:,:6]\n",
        "    fruit = combined_data.iloc[:,6:13]\n",
        "    veg = combined_data.iloc[:,13:20]\n",
        "    lns = combined_data.iloc[:,20:27]\n",
        "    grasses = combined_data.iloc[:,27:34]\n",
        "\n",
        "    # keeping a column for crop type\n",
        "    fruit['Type'] = 'Fruit'\n",
        "    veg['Type'] = 'Veg'\n",
        "    lns['Type'] = 'Legumes Seeds'\n",
        "    grasses['Type'] = 'Grains'\n",
        "\n",
        "    crops = [fruit, veg, lns, grasses]\n",
        "    crops_0 = []\n",
        "    new_col_names = ['Date visited','Auditor','Region','Community','Family visited','Present?',\n",
        "                          'Crop','Seedling_or_transplanted', '% Disease','Condition',\n",
        "                          'Plague','Organic recommendation','Chemical recommendation', 'Type']\n",
        "\n",
        "    for table in crops:\n",
        "      temp = pd.concat([overall_info, table], axis=1)\n",
        "      temp.columns = new_col_names\n",
        "      crops_0.append(temp)\n",
        "\n",
        "    final = crops_0[0]\n",
        "    for table in crops_0[1:]:\n",
        "      final = pd.concat([final, table], axis=0)\n",
        "\n",
        "    # Lauren\n",
        "    # Removing empties\n",
        "    final = final[final['Crop']!=0]\n",
        "\n",
        "    # Cleaning names column\n",
        "\n",
        "    final['Family visited'] = final['Family visited'].str.lower()\n",
        "\n",
        "    final = final.replace(\n",
        "        ['arelis', 'arelis  solis', 'arelis solis',\n",
        "          'arelis soliz', 'areliz solis', 'arlelis solis'], 'arelis solis')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['freddy', 'freddy lanza', 'freddy lanzas',\n",
        "          'freddy lasza', 'freddys campo', 'fredi ', 'fredis ',\n",
        "          'fredis lanza', 'fredy lanzas', 'fredys'], 'freddy lanza')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['helen espinoza', 'hellen', 'hellen espinoza', ], 'hellen espinoza')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['johana', 'johana  salgado','johana salgado', 'johanna salgado', 'yohana salgado'], 'johana salgado')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['juan sandobal', 'juan sandoval'], 'juan sandoval')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['maria jose', 'maria jose roque',\n",
        "          'maria jose roque ', 'mariajose roque', ',maria jose roque', ], 'maria jose roque')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['marvin toval', 'marvin toval padilla'], 'marvin toval padilla')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['naideling', 'naideling vargas', 'naidelyn', 'naidelyn vargas', 'naidelyng', 'naidelyng ', 'naidelyng vargas', 'naydelin', 'naydelin varga', 'naydeling', 'naydeling varga', 'naydeling vargas', 'nayeling varga’, ‘neilyng', 'ávila vargas'\n",
        "    ], 'naydeling vargas')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['nayeli roque','nayelis  roque', 'nayelis roque', 'nayelis roqur', 'nerlyn roque',\n",
        "    ], 'nayelis roque')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['nerligh hernandez', 'nerling henandez', 'nerling hernandez',\n",
        "          'nerlyn hernandez', 'nerlynh hernandez',], 'nerling hernandez')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['rayson membreño', 'reison membreño', 'reison membreńo',\n",
        "          'reysom membreño', 'reyson membrecho', 'reyson membreño',\n",
        "          'reyson membreńo', 'reyson menbreño'], 'reysom membreño')\n",
        "\n",
        "    final = final.replace(\n",
        "        ['yader  morales', 'yader morales', 'yader morales ',\n",
        "          'yadermorales', ], 'yader morales')\n",
        "\n",
        "    final[\"Month visited\"] = final[\"Date visited\"].dt.month\n",
        "    final[\"Year visited\"] = final[\"Date visited\"].dt.year      \n",
        "    return final\n",
        "\n",
        "\n",
        "def clean_gps(families_coordinates):\n",
        "    '''Making families_coordinates names identical to VL farming names'''\n",
        "\n",
        "    families_coordinates['Name'] = families_coordinates['Name'].str[8:].str.lower()\n",
        "\n",
        "    families_coordinates['Name'] = families_coordinates['Name'].str.replace('\\s{2,}', ' ')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['rebeca sequeira'], 'rebeca carolina sequeira morales')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['fátima castillo'], 'maría de fátima castillo')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['yojhana cristina flores'], 'johana cristina altamirano flores')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['karla galeano'], 'karla galiano martínez')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['rita arevalo'], 'rita arévalo mora')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['cristina alvares'], 'cristina alvares solís')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['claudia arevalo'], 'claudia flavia arévalo')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['silvia elena  moran'], 'silvia elena moran')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['cristina avendaño'], 'maria cristina avendaño')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['melania jacaba quiroz'], 'melania jocoba quiroz')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['daisy ramirez'], 'maria deisy ramirez')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['maria eugenia morales'], 'maría eugenia morales')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['ana catalina millón'], 'ana catalina garcía millón')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['oralia ramimez'], 'oralia ramirez')\n",
        "    families_coordinates = families_coordinates.replace(\n",
        "        ['roosvelt donaire'], 'roosevelt donaire')\n",
        "\n",
        "    return families_coordinates\n",
        "\n",
        "def merged_unified(farming, families_coordinates, historic_w):\n",
        "    '''\n",
        "    takes farming dataset, family_coordinates, historic weather\n",
        "    '''\n",
        "\n",
        "    # Merge geolocation on family names\n",
        "    geo_farm = pd.merge(farming, families_coordinates[['Name', 'apienter', 'latitude', 'longitude']], \n",
        "                                  how=\"left\", left_on=\"Family visited\", \n",
        "                                  right_on=\"Name\").drop(columns=['Name'])\n",
        "\n",
        "    # For missing locations, average longitude 12.46 and avg latitude: -86.96 are imputed                               \n",
        "    geo_farm['apienter'].fillna('12.46%-86.96', inplace=True)\n",
        "\n",
        "    \n",
        "    historic_w['date_time'] = pd.to_datetime(historic_w.date_time)\n",
        "\n",
        "    # Merge weather on geolocation\n",
        "    final = pd.merge(geo_farm, historic_w,\n",
        "                        how=\"left\", left_on=[\"Date visited\", \"apienter\"],\n",
        "                        right_on=[\"date_time\", \"location\"])\n",
        "    \n",
        "    # Last column addition\n",
        "    final[\"Month visited\"] = final[\"Date visited\"].dt.month\n",
        "    final[\"Year visited\"] = final[\"Date visited\"].dt.year\n",
        "    \n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg3APq1iETVU"
      },
      "source": [
        "# this cell runs all the defined functions to clean the data\n",
        "\n",
        "combined_data = combined_farming(data_17_19,data_19_20)\n",
        "combined_data = clean_farming(combined_data)\n",
        "families_coordinates = clean_gps(families_coordinates)\n",
        "\n",
        "# Check Farming Dataset names against Correct Name List\n",
        "# extra_names = [name for name in final[\"Family visited\"].unique() if name not in correct_names.iloc[:,3].unique()]\n",
        "# extra_names.sort()\n",
        "\n",
        "# Check Family Coordinates names against Farming Dataset names\n",
        "# [name for name in families_coordinates[\"Name\"].unique() if name not in final[\"Family visited\"].unique()]\n",
        "\n",
        "final = merged_unified(combined_data, families_coordinates, historic_w)\n",
        "# Write to CSV\n",
        "# final.to_csv('VL_farm_geo_w.csv')\n",
        "# final.to_csv(\"/content/drive/MyDrive/VL_farm.csv\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDHgBBZ-EjQP"
      },
      "source": [
        "import pickle \n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(xgboost_model_o, open(filename, 'wb'))\n",
        " \n",
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_train_over, y_train_over)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kf0CJJUkmgK"
      },
      "source": [
        "import pickle\n",
        "!pip install app\n",
        "from flask import Flask, render_template,request\n",
        "import pickle #Initialize the flask App\n",
        "\n",
        "app = Flask('crop_prediction')\n",
        "model =loaded_model\n",
        "\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "import pickle\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "\n",
        "\n",
        "# #default page of our web-app\n",
        "# @app.route('/')\n",
        "# def home():\n",
        "#     return render_template('index.html')\n",
        "\n",
        "# #To use the predict button in our web-app\n",
        "# @app.route('/predict',methods=['POST'])\n",
        "\n",
        "# def predict():\n",
        "#     #For rendering results on HTML GUI\n",
        "#     int_features = [float(x) for x in request.form.values()]\n",
        "#     final_features = [np.array(int_features)]\n",
        "#     prediction = model.predict(final_features)\n",
        "#     output = round(prediction[0], 2) \n",
        "#     return render_template('index.html', prediction_text='CO2    Emission of the vehicle is :{}'.format(output))\n",
        "\n",
        "\n",
        "@app.route('/api',methods=['POST'])\n",
        "def predict():\n",
        "    # Get the data from the POST request.\n",
        "    data = request.get_json(force=True)\n",
        "    # Make prediction using model loaded from disk as per the data.\n",
        "    prediction = model.predict(X_train_over, y_train_over)\n",
        "    # Take the first value of prediction\n",
        "    output = prediction[0]\n",
        "    return jsonify(output)\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=53300, debug=True)\n",
        "# predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUnyqYWHkmoV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANKbYY2mkmrm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuM6m_dUkmvC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_BMzUSzkmxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSlGdhcekmz2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}